---
title: Notes on Regression - Projection
author: Timothy Lin
date: '2017-08-23'
slug: notes-on-regression-projection
categories: []
tags: ["regression", "ols", "notes"]
subtitle: ''
---



<p>This is one of my favourite ways of establishing the traditional OLS formula. I remember being totally amazed when I first found out how to derive the OLS formula in a class on linear algebra. Understanding regression through the perspective of projections also shows the connection between the least squares method and linear algebra. It also gives a nice way of visualising the geometry of the OLS technique.</p>
<p>This set of notes is largely inspired by a section in <a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/index.htm">Gilbert Strang’s course on linear algebra</a>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> I will use the same terminology as in the <a href="/2017/08/16/notes-on-regression-ols/">previous post</a>.</p>
<p>Recall the standard regression model and observe the similarities with the commonly used expression in linear algebra written below:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{y} &amp;= \mathbf{X}\mathbf{\beta} \\
b &amp;= Ax
\end{aligned}
\]</span>
Thus, the OLS regression can be motivated as a means of finding the projection of <span class="math inline">\(\mathbf{y}\)</span> on the space span by <span class="math inline">\(\mathbf{X}\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> Or to put it another way, we want to find the vector <span class="math inline">\(\beta\)</span> that would be the closest to <span class="math inline">\(\mathbf{y}\)</span>.</p>
<div class="figure">
<img src="/img/projection_reg.png" alt="projection" />
<p class="caption">projection</p>
</div>
<p>Notice that <span class="math inline">\((\mathbf{y} - \mathbf{X}\beta)\)</span> is orthogonal to <span class="math inline">\(Span (\mathbf{X})\)</span> i.e. it is in the left nullspace of <span class="math inline">\(\mathbf{X}\)</span>. By the definition of nullspace:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{X}&#39;(\mathbf{y} -\mathbf{X}\hat{\beta}) &amp;= 0 \\
\mathbf{X}&#39;\mathbf{y} &amp;= \mathbf{X}&#39;\mathbf{X}\hat{\beta} \\
\hat{\beta} &amp;= (\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{y}
\end{aligned}
\]</span></p>
<p><strong>Notes</strong>:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathbf{X}\hat{\beta} = \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{y} = P_{x}\)</span> is also known as the orthogonal projection matrix. The matrix is <span class="math inline">\(n~\times~n\)</span> dimension. As given by its name, for any vector <span class="math inline">\(b \in R^{n}\)</span>, <span class="math inline">\(P_{x}b \in Span(X)\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbf{y} - \mathbf{X}\hat{\beta}\)</span> is simply the vector of residuals and can be written in the following form:</p></li>
</ol>
<p><span class="math display">\[
\begin{aligned}
\hat{u} &amp;= \mathbf{y} - \mathbf{X}\hat{\beta} \\
&amp;= \mathbf{y} - P_{x}\mathbf{y} \\
&amp;= (I_{n} - P_{x})\mathbf{y} \\
&amp;= M_{x}\mathbf{y}
\end{aligned}
\]</span></p>
<p><span class="math inline">\(M_{x}\)</span> is the projection onto the space orthogonal to <span class="math inline">\(Span(X)\)</span>.</p>
<ol start="3" style="list-style-type: decimal">
<li>The projection matrices have the following four properties: <span class="math inline">\(P_{x} + M_{x} = I_{n}\)</span>, Symmetry (<span class="math inline">\(A&#39;=A\)</span>), Idempotent (<span class="math inline">\(AA=A\)</span>), Orthogonal (<span class="math inline">\(P_{x}M_{x} = 0\)</span>).</li>
</ol>
<p><strong>Additional Comments:</strong></p>
<ol style="list-style-type: decimal">
<li><p>The idea of seeing fitted values and residuals in terms of projections and orthogonal spaces have further applications in econometrics. See for example the derivation of the partitioned regression formula.</p></li>
<li><p>As a fun exercise one can try to derive the OLS formula for a weighted regression <span class="math inline">\(\mathbf{W}\mathbf{X}\beta = \mathbf{W}\mathbf{y}\)</span> where <span class="math inline">\(\mathbf{W}\)</span> is an <span class="math inline">\(n \times n\)</span> matrix of weights using the same idea.</p></li>
</ol>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>My two favourite sources covering the basics of linear algebra are <a href="http://joshua.smcvt.edu/linearalgebra/">Hefferon’s linear algebra</a>, a free textbook, and Gilbert Strang’s course mentioned above. Hefferon provides a very clear treatment on the more theoretical aspects of the subject, while the latter highlights the many possibilities and applications that one can do with it.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>The span of the vectors in <span class="math inline">\(\mathbf{X}\)</span> (column space) is the set of all vectors in <span class="math inline">\(R^{n}\)</span> that can be written as linear combinations of the columns of <span class="math inline">\(\mathbf{X}\)</span><a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
