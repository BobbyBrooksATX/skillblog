---
title: Thesis Thursday 5 - From recipes to weights
author: Timothy Lin
date: '2017-07-01'
slug: thesis-thursday-5-from-recipes-to-weights
categories: []
tags: ["Thesis Thursday","R"]
subtitle: ''
---



<p>In the <a href="https://www.timlrx.com/2017/06/24/thesis-thursday-4-analysing-recipes/">previous post</a>, I provided an exploratory analysis of the allrecipe dataset. This post is a continuation and details the construction of product weights from the recipe corpus.</p>
<div id="tf-idf" class="section level3">
<h3>TF-IDF</h3>
<p>To obtain a measure of how unique a particular word is to given recipe category, I calculate each word-region score using the TF-IDF approach which is given by the following formula:</p>
<p><span class="math display">\[
TF\text{-}IDF_{t,d} =\frac{f_{t,d}}{\sum_{t&#39;\in d}f_{t&#39;,d}} \cdot log \frac{N}{n_{t}+1}
\]</span>
where <span class="math inline">\(f_{t,d}\)</span> is the frequency in which a term, <span class="math inline">\(t\)</span>, appears in document <span class="math inline">\(d\)</span>, <span class="math inline">\(N\)</span> is the total number of documents in the corpus and <span class="math inline">\(n_{t}\)</span> is the total number of documents where term <span class="math inline">\(t\)</span> is found.</p>
<p>The first term takes the number of times a particular word appears in a document and normalise it against the document length. This is also known as the term frequency (TF). The second term takes the logarithm of the ratio of the total number of documents divided by the frequency which the term is found across all documents in the corpus and is known as the inverse document frequency (IDF).</p>
<p>A term associated with a particular document has a high TF-IDF score if it frequently recurs in the document and is rarely found in other documents.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> As applied in my context, ingredients take the place of terms and geographical regions, documents. One could imagine that a word like kimchi would rank very highly as it is frequently featured in Korean recipes and are rarely found (or non-existent) in other cuisines. On the other hand, common ingredients such as salt and pepper would have a TF-IDF score of 0.</p>
<p>The <code>tidytext</code> package in R makes constructing the TF-IDF score a breeze.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> All it needs is a vector of words, associated with each particular document and the frequency which it is found in the document. The <code>bind_tf_idf</code> function does the rest.</p>
<pre class="r"><code>geog_words &lt;- tidy_recipe %&gt;%
  count(geog, word, sort = TRUE) %&gt;%
  ungroup() %&gt;%
  bind_tf_idf(word, geog, n) %&gt;%
  arrange(desc(tf_idf))</code></pre>
<p>Now we can do a little visualisation of the top TF-IDF scores obtained</p>
<pre class="r"><code>plot_geog &lt;- geog_words %&gt;%
  arrange(desc(tf_idf)) %&gt;%
  mutate(word = factor(word, levels = rev(unique(word))))

plot_geog %&gt;% 
  top_n(20) %&gt;%
  ggplot(aes(word, tf_idf)) +
  geom_col() +
  labs(x = NULL, y = &quot;tf-idf&quot;) +
  coord_flip() +
  theme_classic()</code></pre>
<div class="figure"><span id="fig:topscores"></span>
<img src="/post/2017-07-01-thesis-thursday-5-from-recipes-to-weights_files/figure-html/topscores-1.png" alt="Top 20 TF-IDF scores" width="672" />
<p class="caption">
Figure 1: Top 20 TF-IDF scores
</p>
</div>
<p>Country names dominate the top of the charts while other cuisine specific ingredients (e.g. Kimchi) also rank highly. A plot of the top 10 TF-IDF scores for selected regions also seem to produce sensible results.</p>
<pre class="r"><code>plot_geog %&gt;% 
  filter(geog==&quot;Italy&quot; | geog==&quot;India&quot; |
         geog==&quot;Central_America&quot; | geog==&quot;Middle_East&quot;) %&gt;%
  group_by(geog) %&gt;% 
  arrange(geog, desc(tf_idf)) %&gt;%
  top_n(10) %&gt;% 
  ungroup %&gt;%
  ggplot(aes(word, tf_idf, fill = geog)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = &quot;tf-idf&quot;) +
  facet_wrap(~geog, ncol = 2, scales = &quot;free&quot;) +
  coord_flip() +
  theme_classic()</code></pre>
<div class="figure"><span id="fig:ctyplot"></span>
<img src="/post/2017-07-01-thesis-thursday-5-from-recipes-to-weights_files/figure-html/ctyplot-1.png" alt="Top 10 TF-IDF scores for selected regions" width="672" />
<p class="caption">
Figure 2: Top 10 TF-IDF scores for selected regions
</p>
</div>
<p>Subsequently, I match the TF-IDF scores for each word to the product description given by the Nielsen dataset. The scores are then normalised such that they sum up to one. This gives the metric a probabilistic interpretation, where the weight for a particular product can be thought of as the naive bayes estimate.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p>Using the dataset of household purchases, I am then able to calculate each household geographical region weighted expenditure share, which is the dependent variable in my regression analysis.</p>
</div>
<div id="other-updates" class="section level3">
<h3>Other updates</h3>
<p>On top of working on the actual regression analysis, I have also started collecting a 2nd set of food-region terms. Using two different data sources where the errors are quite likely uncorrelated would give me a better signal of the dependent variable, reducing problems of measurement error.</p>
<p>The next update should be quite a big one and would bring all the previous blog posts together so stay tune (if you are following).</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>If it is found it every document <span class="math inline">\(n_{t}=N\)</span> and <span class="math inline">\(log 1 = 0\)</span><a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>The <code>tm</code> package is another popular choice for text analysis.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>Interestingly, while the TF-IDF approach seems relatively intuitive and sensible, the theoretical groundings for it are far less established. Nonetheless, it has be shown to be far better than traditional naive bayes for text categorisation purposes. See <a href="http://www.cs.waikato.ac.nz/~eibe/pubs/kibriya_et_al_cr.pdf">Kibriya et.al. (2004)</a> for a comparison. <a href="#fnref3" class="footnote-back">↩</a></p></li>
</ol>
</div>
