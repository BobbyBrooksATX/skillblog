---
title: Notes on Graphs and Spectral Properties
author: Timothy Lin
date: '2017-12-25'
slug: notes-on-graphs-and-spectral-properties
categories: []
tags: ["graph theory", "notes"]
---



<p>Here is the first series of a collection of notes which I jotted down over the past 2 months as I tried to make sense of algebraic graph theory. This one focuses on the basic definitions and some properties of matrices related to graphs. Having all the symbols and main properties in a single page is a useful reference as I delve deeper into the applications of the theories. Also, it saves me time from googling and checking the relationship between these objects.</p>
<div id="adjacency-matrix" class="section level2">
<h2>Adjacency Matrix</h2>
<p>Let <span class="math inline">\(n\)</span> be the number of vertices and <span class="math inline">\(m\)</span> the number of edges. Then the adjacency matrix <span class="math inline">\(A\)</span> of dimension <span class="math inline">\(n \times n\)</span> is a matrix where <span class="math inline">\(a_{ij}=1\)</span> where there is an edge from vertex i to vertex j and zero otherwise. For a weighted adjacency matrix, <span class="math inline">\(W\)</span>, we replace 1 with the weights, <span class="math inline">\(w_{ij}\)</span>.</p>
<p>Here we consider the case of undirected graphs. This means that the adjacency matrix is symmetric which implies it has a complete set of real eigenvalues (not necessary positive) and an orthogonal eigenvector basis. The set of eigenvalues (<span class="math inline">\(\alpha_{1} \geq \alpha_{2} \geq ... \geq \alpha_{n}\)</span>) is known as the spectrum of a graph.</p>
<div id="properties" class="section level3">
<h3>Properties</h3>
<ul>
<li>The greatest eigenvalue, <span class="math inline">\(\alpha_{1}\)</span> is bounded by the maximum degree.<br />
</li>
<li>Given two graphs with adjacency matrix <span class="math inline">\(A_{1}\)</span> and <span class="math inline">\(A_{2}\)</span>, the graphs are isomorphic iff there exist a permutation matrix <span class="math inline">\(P\)</span> such that <span class="math inline">\(PA_{1}P^{-1}=A_{2}\)</span>. Implies same eigenvalue /eigenvectors/determinant/trace etc. Note: Two graphs may be isospectral (same set of eigenvalues) but NOT isomorphic.</li>
</ul>
</div>
</div>
<div id="incidence-matrix" class="section level2">
<h2>Incidence Matrix</h2>
<p>An incidence matrix <span class="math inline">\(\tilde{D}\)</span> is of dimension <span class="math inline">\(n \times m\)</span> with <span class="math inline">\(D_{ij}=1\)</span> if <span class="math inline">\(e_{j} = (v_{i},v_{k})\)</span> or <span class="math inline">\(-1\)</span> if<span class="math inline">\(e_{j} = (v_{j},v_{i})\)</span> or zero otherwise. In other words, each column represents an edge that shows the vertex it is emitting from (1) and the vertex it is pointing to (-1).</p>
<p>For an undirected graph, there are two kinds of incidence matrix: oriented and unoriented. In the unoriented graph, we just put 1 for any vertex that is connected to an edge. The unoriented graph is similar to that of a directed graph (1 and -1) and is unique up to the negation of the columns.</p>
</div>
<div id="laplacian-matrix" class="section level2">
<h2>Laplacian Matrix</h2>
<p>Laplacian matrix is defined as <span class="math inline">\(L = D - A = \tilde{D}\tilde{D}&#39;\)</span>, or the degree matrix <span class="math inline">\(D\)</span> minus the adjacency matrix <span class="math inline">\(A\)</span>. Hence, the diagonals are the degree while <span class="math inline">\(L_{ij}=-1\)</span> if <span class="math inline">\(v_{i}\)</span> and <span class="math inline">\(v_{j}\)</span> are connected, else 0.</p>
<p><strong>Note:</strong></p>
<ul>
<li><span class="math inline">\(\tilde{D}\)</span> is the unoriented incidence matrix.<br />
</li>
<li>The degree matrix is defined as <span class="math inline">\(D = diag(W \cdot \mathbf{1})\)</span>.<br />
</li>
<li>For a weighted degree matrix, the diagonal element <span class="math inline">\(d(i,i) = \sum_{j:(i,j)\in E} w_{ij}\)</span>.<br />
</li>
<li>The conventional ordering of eigenvalue is opposite to the adjacency matrix! (<span class="math inline">\(0=\lambda_{1} \leq \lambda_{2} \leq ... \leq \lambda_{n}\)</span>.)</li>
</ul>
<div id="walks" class="section level3">
<h3>Walks</h3>
<p>A walk on a graph is an alternating path of vertex and series from one vertex to another. A walk between two vertices <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> is called a <span class="math inline">\(u-v\)</span> walk. Itâ€™s length is the number of edges.</p>
<p><strong>Cool fact:</strong> Take the adjacency matrix and multiply it <span class="math inline">\(n\)</span> times, then <span class="math inline">\(a^{n}_{ij}\)</span>, an entry from the <span class="math inline">\(A^{n}\)</span> matrix gives the number of <span class="math inline">\(i-j\)</span> walks of length <span class="math inline">\(n\)</span>. Divide the <span class="math inline">\(i,j\)</span> entry by the degree of vertex <span class="math inline">\(i\)</span>. Then the <span class="math inline">\(i,j\)</span> entry would give the probability that starting from <span class="math inline">\(i\)</span>, you will end up at <span class="math inline">\(j\)</span> after <span class="math inline">\(n\)</span> steps.</p>
</div>
<div id="matrices-as-operators-on-the-vertices" class="section level3">
<h3>Matrices as operators on the vertices</h3>
<p>The adjacency and laplacian matrix can be interpreted as operators on functions of a graph. That is, given <span class="math inline">\(Ax\)</span>, <span class="math inline">\(x\)</span> can be interpreted as a function on the vertices, while <span class="math inline">\(A\)</span> is a linear mapping of the function <span class="math inline">\(x\)</span>. <span class="math display">\[
Ax(i) = \sum_{j:(i,j)\in E} x_{j} 
\]</span> Or in other words, it is the sum of the elements of x that are connected to vertex <span class="math inline">\(i\)</span>. It can also be viewed as a quadratic form: <span class="math display">\[
x&#39;Ax = \sum_{e_{ij}} x_{i}x_{j}
\]</span> Similarly, expressing the weighted laplacian matrix as an operator: <span class="math display">\[
\begin{aligned}
Lx(i) &amp;= Dx(i) - Wx(i) \\
&amp;= \sum_{j:(i,j)\in E} w_{ij} x_{i} - \sum_{j:(i,j)\in E} w_{ij}x_{j}  \\
&amp;= \sum_{j:(i,j)\in E} w_{ij}(x_{i}-x_{j})
\end{aligned}
\]</span></p>
<p>As a quadratic form <span class="math display">\[
\begin{aligned}
x&#39;Lx &amp;= x&#39;Dx - x&#39;Wx \\
&amp;= \sum w_{ij}x_{i}^{2} - \sum_{e_{ij}} x_{i}w_{ij}x_{j} \\
&amp;= \frac{1}{2}(\sum w_{ij}x_{i}^{2} - 2\sum_{e_{ij}} x_{i}w_{ij}x_{j} + \sum w_{ij}x_{j}^{2}) \\
&amp;= \frac{1}{2}\sum_{e_{ij}} w_{ij}(x_{i}-x_{j})^{2}
\end{aligned}
\]</span></p>
<p>The symmetric normalised Laplacian matrix is defined as <span class="math inline">\(L^{sym} = D^{-1/2}LD^{-1/2} = I - D^{-1/2}AD^{-1/2}\)</span>.</p>
<p>Since the degree matrix is a diagonal matrix, <span class="math inline">\(D^{-1/2}\)</span> is just the <span class="math inline">\(D\)</span> matrix with the diagonals square rooted.</p>
</div>
<div id="properties-of-l" class="section level3">
<h3>Properties of <span class="math inline">\(L\)</span></h3>
<ul>
<li><span class="math inline">\(L\)</span> is symmetry because <span class="math inline">\(W\)</span> is symmetric.<br />
</li>
<li><span class="math inline">\(\mathbf{1}\)</span> is an eigenvector of the matrix (sum of any column=0), and <span class="math inline">\(L\mathbf{1} = 0\mathbf{1}\)</span>, hence 0 is the smallest eigenvalue.<br />
</li>
<li>The eigenvalues <span class="math inline">\(0=\lambda_{1} \leq \lambda_{2} \leq ... \leq \lambda_{n}\)</span> are real and non-negative.</li>
</ul>
</div>
</div>
<div id="laplacian-matrix-and-connectedness" class="section level2">
<h2>Laplacian Matrix and Connectedness</h2>
<p>Define a path as a walk without any repeated vertices. A graph is connected if any two of its vertices are contained in a path.</p>
<p>In a fully connected graph, <span class="math inline">\(\lambda_{2}&gt;0\)</span>. Proof that the only eigenvector is <span class="math inline">\(\mathbf{1}\)</span>: Let <span class="math inline">\(x\)</span> be the eigenvector associated with the eigenvalue 0. From the quadratic form: <span class="math display">\[
x&#39;Lx = x&#39;0 = 0 = \sum_{e_{ij}} w_{ij}(x_{i}-x_{j})^{2}
\]</span> This implies that for any <span class="math inline">\({i,j} \in E, x_{i} = x_{j}\)</span>. Since, there exist a path from any two vertices, <span class="math inline">\(x_{i} = x_{j}\)</span> for all <span class="math inline">\(i,j \in V\)</span>: <span class="math display">\[
x = \alpha 
\left[\begin{array}{c}
1 \\
1 \\
. \\
. \\
1
\end{array}\right]
\]</span> Hence, the multiplicity (number of linearly independent) of eigenvalue 0 is 1, and <span class="math inline">\(\lambda_{2} &gt; 0\)</span>.</p>
<p>In fact, the multiplicity of the eigenvalue 0 tells us the number of connected components in the graph. For example, a graph with two connected components (the adjacency matrix and the laplacian matrix will have a block diagonal structure), you will get two eigenvectors associated with the eigenvalue 0. Something like <span class="math inline">\([1~1~1~0~0~0]&#39;\)</span> and <span class="math inline">\([0~0~0~1~1~1]&#39;\)</span>.</p>
<p>To summarise, the number of connected components is equal to the multiplicity of eigenvalue 0 which is equal to the dimension of the null space of <span class="math inline">\(L\)</span>.</p>
<div id="normalised-symmetric-laplacian-and-random-walk-matrix" class="section level3">
<h3>Normalised Symmetric Laplacian and Random walk matrix</h3>
<p>The normalised symmetric laplacian is defined as: <span class="math display">\[
L_{sym} = I - D^{-1/2}WD^{-1/2} = D^{-1/2}LD^{-1/2}
\]</span> In other words, it has 1 on the diagonals and <span class="math inline">\(-\frac{1}{\sqrt{deg(v_{i})deg(v_{j})}}\)</span> if <span class="math inline">\(v_{i}\)</span> is adjacent to <span class="math inline">\(v_{j}\)</span> and 0 otherwise.</p>
<p>The random walk matrix is defined as: <span class="math display">\[
L_{rw} = D^{-1}L = I - D^{-1}W = D^{-1/2}L_{sym}D^{1/2}
\]</span> <span class="math inline">\(L_{sym}\)</span> and <span class="math inline">\(L_{rw}\)</span> are similar matrices.</p>
</div>
<div id="properties-of-ll_sym-and-l_rw" class="section level3">
<h3>Properties of <span class="math inline">\(L\)</span>,<span class="math inline">\(L_{sym}\)</span> and <span class="math inline">\(L_{rw}\)</span></h3>
<ul>
<li>The three matrices are symmetric, positive, semidefinite.<br />
</li>
<li><span class="math inline">\(L_{sym}\)</span> and <span class="math inline">\(L_{rw}\)</span> share the same eigenvalues. <span class="math inline">\(u\)</span> is an eigenvector of <span class="math inline">\(L_{rw}\)</span> iff <span class="math inline">\(D^{1/2}u\)</span> is an eigenvector of <span class="math inline">\(L_{sym}\)</span>.<br />
</li>
<li><span class="math inline">\(u\)</span> is a solution of the eigenvalue problem <span class="math inline">\(Lu = \lambda Du\)</span> iff <span class="math inline">\(D^{1/2}u\)</span> is an eigenvector of <span class="math inline">\(L_{sym}\)</span> for the eigenvalue <span class="math inline">\(\lambda\)</span> iff <span class="math inline">\(u\)</span> is an eigenvector of <span class="math inline">\(L_{rw}\)</span> for the eigenvalue <span class="math inline">\(\lambda\)</span> .</li>
<li>A similar connection between the connected components and <span class="math inline">\(L\)</span> can be made with <span class="math inline">\(L_{sym}\)</span> and <span class="math inline">\(L_{rw}\)</span>.</li>
</ul>
</div>
</div>
