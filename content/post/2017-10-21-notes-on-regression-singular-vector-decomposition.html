---
title: Notes on Regression - Singular Vector Decomposition
author: Timothy Lin
date: '2017-10-21'
slug: notes-on-regression-singular-vector-decomposition
categories: []
tags: ["regression", "ols", "notes"]
subtitle: ''
---



<p>Here’s a fun take on the OLS that I picked up from <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">The Elements of Statistical Learning</a>. It applies the Singular Value Decomposition, also known as the method used in principal component analysis, to the regression framework.</p>
<div id="singular-vector-decomposition-svd" class="section level3">
<h3>Singular Vector Decomposition (SVD)</h3>
<p>First, a little background on the SVD. The SVD could be thought of as a generalisation of the eigendecomposition. An eigenvector v of matrix <span class="math inline">\(\mathbf{A}\)</span> is a vector that is mapped to a scaled version of itself: <span class="math display">\[
\mathbf{A}v = \lambda v
\]</span> where <span class="math inline">\(\lambda\)</span> is known as the eigenvalue. For a full rank matrix (this guarantees orthorgonal eigenvectors), we can stack up the eigenvalues and eigenvectors (normalised) to obtain the following equation: <span class="math display">\[
\begin{aligned}
\mathbf{A}\mathbf{Q} &amp;= \mathbf{Q}\Lambda  \\
\mathbf{A} &amp;= \mathbf{Q}\Lambda\mathbf{Q}^{-1}
\end{aligned}
\]</span> where <span class="math inline">\(\mathbf{Q}\)</span> is an orthonormal matrix.</p>
<p>For the SVD decomposition, <span class="math inline">\(\mathbf{A}\)</span> can be any matrix (not square). The trick is to consider the square matrices <span class="math inline">\(\mathbf{A}&#39;\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{A}\mathbf{A}&#39;\)</span>. The SVD of the <span class="math inline">\(n \times k\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span> is <span class="math inline">\(\mathbf{U}\mathbf{D}\mathbf{V}&#39;\)</span>, where <span class="math inline">\(\mathbf{U}\)</span> is a square matrix of dimension <span class="math inline">\(n\)</span> and <span class="math inline">\(\mathbf{V}\)</span> is a square matrix of dimension <span class="math inline">\(k\)</span>. This implies that <span class="math inline">\(\mathbf{A}&#39;\mathbf{A} = \mathbf{V}\mathbf{D}^{2}\mathbf{V}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> can be seen to be the eigenvalue matrix of that square matrix. Similarly, the eigenvectors of <span class="math inline">\(\mathbf{A}\mathbf{A}&#39;\)</span> forms the columns of <span class="math inline">\(\mathbf{U}\)</span> while <span class="math inline">\(\mathbf{D}\)</span> is the square root of the eigenvalues of either matrix.</p>
<p>In practice, there is no need to calculate the full set of eigenvectors for both matrices. Assuming that the rank of <span class="math inline">\(\mathbf{A}\)</span> is k, i.e. it is a long matrix, there is no need to find all n eigenvectors of <span class="math inline">\(\mathbf{A}\mathbf{A}&#39;\)</span> since only the elements of the first k eigenvalues will be multiplied by non-zero elements. Hence, we can restrict <span class="math inline">\(\mathbf{U}\)</span> to be a <span class="math inline">\(n \times k\)</span> matrix and let <span class="math inline">\(\mathbf{D}\)</span> be a <span class="math inline">\(k \times k\)</span> matrix.</p>
<p>Here’s a paint illustration of the dimensions of the matrices produced by the SVD decomposition to illustrate the idea more clearly. The <span class="math inline">\(n \times k\)</span> matrix produced by multiplying <span class="math inline">\(\mathbf{U}\)</span> and <span class="math inline">\(\mathbf{D}\)</span> is equivalent for both the blue (keeping all n eigenvectors) and red (keeping only the relevant k eigenvectors) boxes. <img src="/img/SVD_dimension.png" alt="svd" /></p>
</div>
<div id="applying-the-svd-to-ols" class="section level3">
<h3>Applying the SVD to OLS</h3>
<p>To apply the SVD to the OLS formula, we re-write the fitted values, substituting the data input matrix <span class="math inline">\(X\)</span> with its equivalent decomposed matrices:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{X}\hat{\beta} &amp;= \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{y} \\
&amp;= \mathbf{U}\mathbf{D}\mathbf{V}&#39;(\mathbf{V}\mathbf{D}&#39;\mathbf{U}&#39;\mathbf{U}\mathbf{D}\mathbf{V}&#39;)^{-1}\mathbf{V}\mathbf{D}&#39;\mathbf{U}&#39;\mathbf{y} \\
&amp;= \mathbf{U}\mathbf{D}(\mathbf{D}&#39;\mathbf{D})^{-1}\mathbf{D}\mathbf{U}&#39;\mathbf{y} \\
&amp;= \mathbf{U}\mathbf{U}&#39;\mathbf{y}
\end{aligned} 
\]</span> where the third to fourth line comes from the fact that <span class="math inline">\((\mathbf{D}&#39;\mathbf{D})^{-1}\)</span> is a <span class="math inline">\(k \times k\)</span> matrix with the square root of the eigenvalues on the diagonal, and <span class="math inline">\(\mathbf{D}\)</span> is a square diagonal matrix. Here we see that the fitted values are computed with respect to the orthonormal basis <span class="math inline">\(\mathbf{U}\)</span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
</div>
<div id="link-to-the-ridge-regression" class="section level3">
<h3>Link to the ridge regression</h3>
<p>The ridge regression is an OLS regression with an additional penalty term on the size of the coefficients and is a popular model in the machine learning literature. In other words, the parameters are chosen to minimalise the penalised sum of squares: <span class="math display">\[
\sum_{i=1}^{n}(y_{i} - \sum_{j=1}^{k} x_{ij}\beta_{j})^{2} + \lambda \sum_{j=1}^{k} \beta_{j}^{2}
\]</span> The solution to the problem is given by: <span class="math inline">\(\hat{\beta}^{ridge} = (\mathbf{X}&#39;\mathbf{X} + \lambda I_{k})^{-1}\mathbf{X}&#39;\mathbf{Y}\)</span>. Substituting the SVD formula into the fitted values of the ridge regression:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{X}\hat{\beta}^{ridge} &amp;= \mathbf{X}(\mathbf{X}&#39;\mathbf{X} + \lambda\mathbf{I})^{-1}\mathbf{X}&#39;\mathbf{y} \\
&amp;= \mathbf{U}\mathbf{D}(\mathbf{D}&#39;\mathbf{D} + \lambda\mathbf{I})^{-1}\mathbf{D}\mathbf{U}&#39;\mathbf{y} \\
&amp;= \sum_{j=1}^{k} \mathbf{u}_{j} \frac{d^{2}_{j}}{d^{2}_{j} + \lambda} \mathbf{u}_{j}&#39;\mathbf{y}
\end{aligned} 
\]</span> where <span class="math inline">\(\mathbf{u}\)</span> is a n-length vector from the columns of <span class="math inline">\(\mathbf{U}\)</span>. This formula makes the idea of regularisation really clear. It shrinks the predicted values by the factor <span class="math inline">\(d^{2}_{j}/(d^{2}_{j} + \lambda)\)</span>. Moreover, a greater shrinkage factor is applied to the variables which explain a lower fraction of the variance of the data i.e. lower <span class="math inline">\(d_{j}\)</span>. This comes from the fact that the eigenvectors associated with a higher eigenvalue explain a greater fraction of the variance of the data (see Principal Component Analysis).</p>
<p>The difference between how regularisation works when one uses the Principal Component Analysis (PCA) method vs the ridge regression also becomes clear with the above formulation. The PCA approach truncates variables that fall below a certain threshold, while the ridge regression applies a weighted shrinkage method.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Doing a QR decomposition will also give a similar set of results, though the orthogonal bases will be different.<a href="#fnref1">↩</a></p></li>
</ol>
</div>
